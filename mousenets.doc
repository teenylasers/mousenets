
@title{Mousenets}
@author{teenylasers}

@contents{}



#######################################################################
#######################################################################



@section{Multilayer Perceptron}

@subsection{Notations}


@subsection{Network structure}

A multilayer perceptron (MLP) is a fully connected network that sequentially consists of the input layer, the hidden layer(s), and the output layer. Each hidden or output layer is given by

@M{
  H^{(i)} &= W^{(i)}X^{(i-1)} \\
  X^{(i)} &= F^{(i)}(H^{(i)})
}
for the @m{i}-th layer, where @m{X^{(i-1)}} is the output from the previous layer and the input to the current one, @m{W^{(i)}} is the weights matrix, and @m{F^{(i)}} is the activation function, typically sigmoid, ReLU, or softmax. A loss function is used to compare the MLP output @m{X^(n)} with the ground truth @m{Y} and quantify its @emph{goodness},

@M{
  \mathcal{L}(X^{(n)}, Y) : \mathbb{R}^K \rightarrow \mathbb{R}
}

We use superscripts in parentheses to denote layer number, and subscripts for matrix manipulation using Einstein summation notation.

When we look at a layer on its own, we simplify the notation by ridding of the superscript, and rename the layer output @m{Y},
@M{
  H &= WX \\
  Y &= F(H)
}
where @m{Y} is an @m{N}-dimensional vector, @m{X} is an @m{N_x}-dimensional vector, and @m{W} is an @m{(N \times N_x)} matrix.

@subsubsection{MLP(32,10) for MNIST}

For a first implementation to classify hand-written digits using the MNIST dataset, we use a 3-layer MLP. Each input image is @m{28\times28} pixels, converted to a 784-element vector for the input layer. The single hidden layer is 32-wide, using the sigmoid activation function. The output layer is 10-wide, using the softmax activation function. Therefore,

@M{
  X_0 &\in \mathbb{R}^{784 \times 1} \nonumber \\
  W_1 &\in \mathbb{R}^{785 \times 32} \nonumber \\
  X_1 &\in \mathbb{R}^{32 \times 1} \nonumber \\
  W_2 &\in \mathbb{R}^{33 \times 10} \nonumber \\
  X_2 &\in \mathbb{R}^{10 \times 1}
}

Categorical cross-entropy is used to calculate loss.

@@@@@@ Add bias and its role

@@@@@@ What is backpropagation results supposed to look like, how to check for correctness, how to visualize?

@@@@@@ Learning rate: do you apply only at the first dLdy or at each layer in backpropagation? learning rate schedule?

@@@@@@ Normalization: currently normalize weights and input data only.

@@@@@@ Regularization


@subsection{Activation function}

@subsubsection{Sigmoid function}
@M{
  \sigma(H) &= \frac{1}{1+e^H} \label{eq:sigmoid} \\

  \frac{\partial\sigma}{\partial H} &= \sigma (1-\sigma)
}

@subsubsection{Softmax function}
@M{
  s_j(H) &= \frac{e^{H_j}}{\sum_i^{K} e^{H_i}} \label{eq:softmax} \\

  \frac{\partial s_j}{\partial H_k}
    &= \begin{cases}
       \frac{e^{H_k}(\sum_l e^{H_l} - e^{H_k})}{(\sum_l e^{H_l})^2}, & j=k \\
       \frac{-e^{H_j + H_k}}{(\sum_l e^{H_l})^2}, & j \neq k
       \end{cases} \nonumber \\
    &= \begin{cases}
       s(H_k)(1-s(H_k)), & j=k \\
       \frac{-e^{H_j + H_k}}{(\sum_l e^{H_l})^2}, & j \neq k
       \end{cases} \nonumber \\
    &= \text{Diag}(s) - ss^T \label{eq:softmax_grad}
}


@subsection{Loss function}

@subsubsection{Categorial cross-entropy loss}

Softmax function is applied to interpret neural network output as the probability that the input classifies to an output class @m{j}, @m{P(Y_j | X)}. The sum of the probabilities across the classes is 1, thus the input must be of one of the output categories. Apply maximum likelihood estimation, we can minimize the negative log-likelihood, which is equivalent to maximizing the probability. Thus the cross-entropy loss function becomes

@M{
  \mathcal{L}_{CE} &= -\sum_i Y_i \log(X^{(n)}_i) \label{eq:cross_entropy_loss} \\

  \frac{\partial\mathcal{L}_{CE}}{\partial X} &=  -\frac{Y}{X}
}

When @m{Y} is a one-hot encoded vector, categorial cross-entropy loss is
@M{
  \mathcal{L}_{CCE} &= -\log(x_p) \\

  \frac{\partial \mathcal{L}_{CCE}}{\partial X_j}
    &= \begin{cases}
      0, & Y_j = 0 \\
      -\frac{Y_j}{X_j}, & Y_j = 1
      \end{cases}
}

When categorical cross-entropy loss is combined with softmax function as the activation function of the last layer, we can combine the loss gradient calculation into a single simplified calculation. We substitute @eqref{softmax} into @eqref{cross_entropy_loss}
@M{
  \mathcal{L}_{SCE}(H)
    &= -\sum_i Y_i \log \frac{e^{H_i}}{\sum_k e^{H_k}} \nonumber \\
    &= -\sum_i Y_i \log e^{H_i} + \sum_i Y_i \log \sum_k e^{H_k} \nonumber \\
    &= -\sum_i Y_i H_i + \log \sum_k e^{H_k} \label{eq:loss_sce} \\

  \frac{\partial \mathcal{L}_{SCE}}{\partial H_i}
    &= -Y_i + \frac{e^{H_i}}{\sum_k e^{H_k}} \nonumber \\
    &= -Y_i + \text{softmax}(H_i) \label{eq:loss_grad_sce}
}

@eqref{loss_sce} and @eqref{loss_grad_sce} simplify backpropagation calculations in gradient descent, letting us bypass the somewhat complex @eqref{softmax_grad}.


@subsection{Backpropagation}

We want to minimize the loss function @m{\mathcal{L}} using gradient descent. From the neural net's output @m{X_n} and the ground truth @m{Y}, we can calculate how @m{\mathcal{L}} changes due to each component of @m{X_n}, i.e. the gradient @m{\partial\mathcal{L} / \partial X_n}. We apply the chain rule and work the loss gradient backwards through the net. Writing out the last two layers, we have

@table{

  @* @M*{H^{(n-1)} = W^{(n-1)}X^{(n-2)}}
    @|
    @|

  @* @M*{X^{(n-1)} = F^{(n-1)}(H^{(n-1)}) \nonumber}
    @| @M*{\frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}}
    @| @M*{\frac{\partial\mathcal{L}}{\partial H^{(n-1)}} = \frac{\partial\mathcal{L}}{\partial X^{(n-1)}} \frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}}

  @* @M*{H^{(n)} = W^{(n)}X^{(n-1)}}
    @| @M*{\frac{\partial H^{(n)}}{\partial W^{(n)}}, \frac{\partial H^{(n)}}{\partial X^{(n-1)}}}
    @| @M*{
      \frac{\partial\mathcal{L}}{\partial W^{(n)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
          \frac{\partial H^{(n)}}{\partial W^{(n)}},
      \frac{\partial\mathcal{L}}{\partial X^{(n-1)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
	  \frac{\partial H^{(n)}}{\partial X^{(n-1)}}}

  @* @M*{X^{(n)} = F^{(n)}(H^{(n)})}
    @| @M*{\frac{\partial X^{(n)}}{\partial H^{(n)}} = F'^{(n)}(H^{(n)})}
    @| @M*{
      \frac{\partial\mathcal{L}}{\partial H^{(n)}}
        = \frac{\partial \mathcal{L}}{\partial X^{(n)}}
	  \frac{\partial X^{(n)}}{\partial H^{(n)}}}

  @* @M*{\mathcal{L}(X^{(n)}, Y)}
    @|
    @| @M*{\frac{\partial\mathcal{L}}{\partial X^{(n)}}}

}

Thus, for each @m{i}-th layer, backpropagation is given by
@M{
  \frac{\partial \mathcal{L}}{\partial W^{(i)}}
    &= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       \left(\frac{\partial H^{(i)}}{\partial W^{(i)}}\right)^T \nonumber \\
    &= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       (X^{(i-1)})^T
    \label{eq:backprop_weights} \\
  (N \times N_x) &= (N \times N) (N \times 1) (1 \times N_x) \nonumber
}
@M{
  \frac{\partial \mathcal{L}}{\partial X^{(i-1)}}
    &= \left(\frac{\partial H^{(i)}}{\partial X^{(i-1)}}\right)^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}} \nonumber \\
    &= (W^{(i)})^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
    \label{eq:backprop_x} \\
  (N_x \times 1) &= (N_x \times N) (N \times N) (N \times 1) \nonumber
}
where @eqref{backprop_weights} updates the weights in this layer and @eqref{backprop_x} propagates the loss error to the previous layer. @eqref{backprop_weights} and @eqref{backprop_x} are written in the form to perform backpropagation using matrix operations. For checking matrix dimensions, @m{N} is the width, i.e. the number of nodes in this layer, it is also the width of the layer output; @m{N_x} is the width of the input to this layer.



@subsubsection{Check gradient}

Backpropagation @eqref{backprop_weights} and @eqref{backprop_x} gives us analytically calculated gradient at each layer of the neural network. We can check the correctness of our implementation against numerically calculate gradients, that is, for the @m{i}-th layer, we wish to verify the loss gradient with respect to the layer input @m{\frac{\partial \mathcal{L}}{\partial X^{(i-1)}}} and the weights @m{\frac{\partial \mathcal{L}}{\partial W^{(i)}}}

We can perform gradient check at each layer individually, calculating @m{\frac{\partial \mathcal{L}}{\partial X}} and @m{\frac{\partial \mathcal{L}}{\partial W}} from @m{\frac{\partial \mathcal{L}}{\partial Y}},
@M{
  \frac{\partial \mathcal{L}}{\partial X_k}
    &= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{\partial Y_j}{\partial X_k} \nonumber \nonumber \\
    &= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{Y_j(X_k + \epsilon) - Y_j(X_k - \epsilon)}{2\epsilon}
}

Alternatively, we can check gradient by tweaking the @m{k}-th element on the @m{i}-th layer in the neural net @m{X^{(i)}_k}, then forward propagate its effect through the network all the way to the loss function @m{\mathcal{L}} to calculate the loss gradient,
@M{
  \frac{\partial \mathcal{L}}{\partial X^{(i)}_k}
    &= \frac{\mathcal{L}(X^{(i)}_k + \epsilon) - \mathcal{L}(X^{(i)}_k - \epsilon)}{2\epsilon}
}

@subsection{Optimization}

@subsubsection{Gradient descent: stochastic and batch}

The loss function is defined as an evaluation over the whole data set. From that point of view, we would only backpropagate loss gradient after the entire data set. However, this proves to be inefficient; we need not look at the entire data set to get a good estimate of the loss gradient and update the weights. The opposite strategy is to use @emph{stochastic gradient descent}, where the weights are updates after each sample. This strategy can be noisy. A middle ground is to use minibatches, where we average over enough samples to smooth out the per-sample noise, yet not so many that the advantage of averaging has diminishing returns.

To implement batch gradient descent, we backpropagate the loss gradient for each sample, calculate and accumulate @m{\frac{\partial \mathcal{L}}{\partial W}}. At the end of each batch, we average @m{\frac{\partial \mathcal{L}}{\partial W}} over the batch size and update the weights.

Alternatively, some implementations cache @m{X^{(i)}} for all samples, and perform backpropagation once over the @m{(N_x \times \text{batch_size})} @m{X^{(i)}} matrices. This requires more memory to save intermediate results and matrix by matrix operations for analytical gradient calculations.


@section{Appendix}

@subsection{Matrix derivatives}

@subsubsection{Matrix-vector multiplication}

@M{
  y_{n \times 1} = A_{n \times m}x_{m \times 1} \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
}
@M{
  \frac{\partial f}{\partial x_j}
    = \frac{\partial f}{\partial y_i} \frac{\partial y_i}{\partial x_j}
    = A_{ji} \frac{\partial f}{\partial y_i}
}
@M{
  \frac{\partial f}{\partial x} &= A^T \frac{\partial f}{\partial y} \\
  (m \times 1) &= (m \times n)(n \times 1) \nonumber
}

@subsubsection{Elementwise vector function}
@M{
  y_{n \times 1} = \omega(x_{n \times 1}) \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
}
@M{
  \frac{\partial y_i}{\partial x_j}
    = \begin{cases}
      \omega'(x_j), & i=j \\
      0, & i \neq j
      \end{cases}
    = \text{diagonal matrix}
}
@M{
  \frac{\partial f}{\partial x_j}
    = \frac{\partial y_i}{\partial x_j} \frac{\partial f}{\partial y_i}
}
@M{
  \frac{\partial f}{\partial x} &= \omega'(x) \frac{\partial f}{\partial y} \\
  (n \times 1) &= (n \times n)(n \times 1) \nonumber
}

@subsubsection{Matrix-matrix multiplication}

@M{
  Y_{m \times l} &= A_{m \times n}X_{n \times l}
}
@M{
  f(Y) : \mathbb{R}^{m \times l} \mapsto \mathbb{R}
}
Using subscript-summation nnotation to derive the gradients @m{\frac{\partial f}{\partial X}} and @m{\frac{\partial f}{\partial A}}
@M{
  \frac{\partial f}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{jr} A_{iq}
    = \frac{\partial f}{\partial Y_{ir}} A_{iq}
    = A_{iq} \frac{\partial f}{\partial Y_{ir}}
}
@M{
  \frac{\partial f}{\partial X} = A^T \frac{\partial f}{\partial Y}
}

@M{
  \frac{\partial f}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{iq} X_{rj}
    = \frac{\partial f}{\partial Y_{qj}} X_{rj}
}
@M{
  \frac{\partial f}{\partial A} = \frac{\partial f}{\partial Y} X^T
}

@subsubsection{Elementwise matrix function}
@M{
  Y_{n \times m} = \omega(X_{n \times m}) \\
  f(Y) : \mathbb{R}^{n \times m} \mapsto \mathbb{R}
}
@M{
  \frac{\partial Y_{ij}}{\partial X_{qr}}
    = \begin{cases}
      \omega'(X_{qr}), & (i = q) \& (j = r) \\
      0, & (i \neq q) \| (j \neq r)
      \end{cases}
}
@M{
  \frac{\partial f}{\partial X_{ij}} = \frac{\partial f}{\partial Y_{ij}} \omega'(X_{ij})
}