
@title{Mousenets}
@author{teenylasers}

@contents{}



##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##



@section{Multilayer Perceptron}

@subsection{Notes to sort and add}

@list{
@* Add bias and its role

@* What is backpropagation results supposed to look like, how to check for correctness, how to visualize?

@* Learning rate: do you apply only at the first dLdy or at each layer in backpropagation? learning rate schedule?

@* Normalization: currently normalize weights and input data only.

@* Regularization

@* Dropout layers
}

@subsection{Architecture}

A multilayer perceptron (MLP) is a fully connected network that sequentially consists of the input layer, the hidden layer(s), and the output layer. Each hidden or output layer is given by

@M{
  H^{(i)} &= W^{(i)}X^{(i-1)} \\
  X^{(i)} &= F^{(i)}(H^{(i)})
}
for the @m{i}-th layer, where @m{X^{(i-1)}} is the output from the previous layer and the input to the current one, @m{W^{(i)}} is the weights matrix, and @m{F^{(i)}} is the activation function, typically sigmoid, ReLU, or softmax. A loss function is used to compare the MLP output @m{X^(n)} with the ground truth @m{Y} and quantify its @emph{goodness},

@M{
  \mathcal{L}(X^{(n)}, Y) : \mathbb{R}^K \rightarrow \mathbb{R}
}

We use superscripts in parentheses to denote layer number, and subscripts for matrix manipulation using Einstein summation notation.

When we look at a layer on its own, we simplify the notation by ridding of the superscript, and rename the layer output @m{Y},
@M{
  H &= WX \\
  Y &= F(H)
}
where @m{Y} is an @m{N}-dimensional vector, @m{X} is an @m{N_x}-dimensional vector, and @m{W} is an @m{(N \times N_x)} matrix.

@subsection{Example implementations}

@subsubsection{@label{mlp32_10} MLP(32,10) for MNIST}

For a first implementation to classify hand-written digits using the MNIST dataset, we use a 3-layer MLP. Each input image is @m{28\times28} pixels, converted to a 784-element vector for the input layer. The single hidden layer is 32-wide, using the sigmoid activation function. The output layer is 10-wide, using the softmax activation function. Therefore,

@M{
  X_0 &\in \mathbb{R}^{784 \times 1} \nonumber \\
  W_1 &\in \mathbb{R}^{785 \times 32} \nonumber \\
  X_1 &\in \mathbb{R}^{32 \times 1} \nonumber \\
  W_2 &\in \mathbb{R}^{33 \times 10} \nonumber \\
  X_2 &\in \mathbb{R}^{10 \times 1}
}

Categorical cross-entropy is used to calculate loss.


@subsection{Activation function}

@subsubsection{Sigmoid function}
@M{
  \sigma(H) &= \frac{1}{1+e^H} \label{eq:sigmoid} \\

  \frac{\partial\sigma}{\partial H} &= \sigma (1-\sigma)
}

@subsubsection{Softmax function}
@M{
  s_j(H) &= \frac{e^{H_j}}{\sum_i^{K} e^{H_i}} \label{eq:softmax} \\

  \frac{\partial s_j}{\partial H_k}
    &= \begin{cases}
       \frac{e^{H_k}(\sum_l e^{H_l} - e^{H_k})}{(\sum_l e^{H_l})^2}, & j=k \\
       \frac{-e^{H_j + H_k}}{(\sum_l e^{H_l})^2}, & j \neq k
       \end{cases} \nonumber \\
    &= \begin{cases}
       s(H_k)(1-s(H_k)), & j=k \\
       \frac{-e^{H_j + H_k}}{(\sum_l e^{H_l})^2}, & j \neq k
       \end{cases} \nonumber \\
    &= \text{Diag}(s) - ss^T \label{eq:softmax_grad}
}


@subsection{Loss function}

@subsubsection{Categorial cross-entropy loss}

Softmax function is applied to interpret neural network output as the probability that the input classifies to an output class @m{j}, @m{P(Y_j | X)}. The sum of the probabilities across the classes is 1, thus the input must be of one of the output categories. Apply maximum likelihood estimation, we can minimize the negative log-likelihood, which is equivalent to maximizing the probability. Thus the cross-entropy loss function becomes

@M{
  \mathcal{L}_{CE} &= -\sum_i Y_i \log(X^{(n)}_i) \label{eq:cross_entropy_loss} \\

  \frac{\partial\mathcal{L}_{CE}}{\partial X} &=  -\frac{Y}{X}
}

When @m{Y} is a one-hot encoded vector, categorial cross-entropy loss is
@M{
  \mathcal{L}_{CCE} &= -\log(x_p) \\

  \frac{\partial \mathcal{L}_{CCE}}{\partial X_j}
    &= \begin{cases}
      0, & Y_j = 0 \\
      -\frac{Y_j}{X_j}, & Y_j = 1
      \end{cases}
}

When categorical cross-entropy loss is combined with softmax function as the activation function of the last layer, we can combine the loss gradient calculation into a single simplified calculation. We substitute @eqref{softmax} into @eqref{cross_entropy_loss}
@M{
  \mathcal{L}_{SCE}(H)
    &= -\sum_i Y_i \log \frac{e^{H_i}}{\sum_k e^{H_k}} \nonumber \\
    &= -\sum_i Y_i \log e^{H_i} + \sum_i Y_i \log \sum_k e^{H_k} \nonumber \\
    &= -\sum_i Y_i H_i + \log \sum_k e^{H_k} \label{eq:loss_sce} \\

  \frac{\partial \mathcal{L}_{SCE}}{\partial H_i}
    &= -Y_i + \frac{e^{H_i}}{\sum_k e^{H_k}} \nonumber \\
    &= -Y_i + \text{softmax}(H_i) \label{eq:loss_grad_sce}
}

@eqref{loss_sce} and @eqref{loss_grad_sce} simplify backpropagation calculations in gradient descent, letting us bypass the somewhat complex @eqref{softmax_grad}.


@subsection{Backpropagation}

We want to minimize the loss function @m{\mathcal{L}} using gradient descent. From the neural net's output @m{X_n} and the ground truth @m{Y}, we can calculate how @m{\mathcal{L}} changes due to each component of @m{X_n}, i.e. the gradient @m{\partial\mathcal{L} / \partial X_n}. We apply the chain rule and work the loss gradient backwards through the net. Writing out the last two layers, we have

@table{

  @* @M*{H^{(n-1)} = W^{(n-1)}X^{(n-2)}}
    @|
    @|

  @* @M*{X^{(n-1)} = F^{(n-1)}(H^{(n-1)}) \nonumber}
    @| @M*{\frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}}
    @| @M*{\frac{\partial\mathcal{L}}{\partial H^{(n-1)}} = \frac{\partial\mathcal{L}}{\partial X^{(n-1)}} \frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}}

  @* @M*{H^{(n)} = W^{(n)}X^{(n-1)}}
    @| @M*{\frac{\partial H^{(n)}}{\partial W^{(n)}}, \frac{\partial H^{(n)}}{\partial X^{(n-1)}}}
    @| @M*{
      \frac{\partial\mathcal{L}}{\partial W^{(n)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
          \frac{\partial H^{(n)}}{\partial W^{(n)}},
      \frac{\partial\mathcal{L}}{\partial X^{(n-1)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
	  \frac{\partial H^{(n)}}{\partial X^{(n-1)}}}

  @* @M*{X^{(n)} = F^{(n)}(H^{(n)})}
    @| @M*{\frac{\partial X^{(n)}}{\partial H^{(n)}} = F'^{(n)}(H^{(n)})}
    @| @M*{
      \frac{\partial\mathcal{L}}{\partial H^{(n)}}
        = \frac{\partial \mathcal{L}}{\partial X^{(n)}}
	  \frac{\partial X^{(n)}}{\partial H^{(n)}}}

  @* @M*{\mathcal{L}(X^{(n)}, Y)}
    @|
    @| @M*{\frac{\partial\mathcal{L}}{\partial X^{(n)}}}

}

Thus, for each @m{i}-th layer, backpropagation is given by
@M{
  \frac{\partial \mathcal{L}}{\partial W^{(i)}}
    &= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       \left(\frac{\partial H^{(i)}}{\partial W^{(i)}}\right)^T \nonumber \\
    &= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       (X^{(i-1)})^T
    \label{eq:backprop_weights} \\
  (N \times N_x) &= (N \times N) (N \times 1) (1 \times N_x) \nonumber
}
@M{
  \frac{\partial \mathcal{L}}{\partial X^{(i-1)}}
    &= \left(\frac{\partial H^{(i)}}{\partial X^{(i-1)}}\right)^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}} \nonumber \\
    &= (W^{(i)})^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
    \label{eq:backprop_x} \\
  (N_x \times 1) &= (N_x \times N) (N \times N) (N \times 1) \nonumber
}
where @eqref{backprop_weights} updates the weights in this layer and @eqref{backprop_x} propagates the loss error to the previous layer. @eqref{backprop_weights} and @eqref{backprop_x} are written in the form to perform backpropagation using matrix operations. For checking matrix dimensions, @m{N} is the width, i.e. the number of nodes in this layer, it is also the width of the layer output; @m{N_x} is the width of the input to this layer.



@subsubsection{Check gradient}

Backpropagation @eqref{backprop_weights} and @eqref{backprop_x} gives us analytically calculated gradient at each layer of the neural network. We can check the correctness of our implementation against numerically calculate gradients, that is, for the @m{i}-th layer, we wish to verify the loss gradient with respect to the layer input @m{\frac{\partial \mathcal{L}}{\partial X^{(i-1)}}} and the weights @m{\frac{\partial \mathcal{L}}{\partial W^{(i)}}}

We can perform gradient check at each layer individually, calculating @m{\frac{\partial \mathcal{L}}{\partial X}} and @m{\frac{\partial \mathcal{L}}{\partial W}} from @m{\frac{\partial \mathcal{L}}{\partial Y}},
@M{
  \frac{\partial \mathcal{L}}{\partial X_k}
    &= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{\partial Y_j}{\partial X_k} \nonumber \nonumber \\
    &= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{Y_j(X_k + \epsilon) - Y_j(X_k - \epsilon)}{2\epsilon}
}

Alternatively, we can check gradient by tweaking the @m{k}-th element on the @m{i}-th layer in the neural net @m{X^{(i)}_k}, then forward propagate its effect through the network all the way to the loss function @m{\mathcal{L}} to calculate the loss gradient,
@M{
  \frac{\partial \mathcal{L}}{\partial X^{(i)}_k}
    &= \frac{\mathcal{L}(X^{(i)}_k + \epsilon) - \mathcal{L}(X^{(i)}_k - \epsilon)}{2\epsilon}
}

@subsection{Optimization}

@subsubsection{Gradient descent: stochastic and batch}

The loss function is defined as an evaluation over the whole data set. From that point of view, we would only backpropagate loss gradient after the entire data set. However, this proves to be inefficient; we need not look at the entire data set to get a good estimate of the loss gradient and update the weights. The opposite strategy is to use @emph{stochastic gradient descent}, where the weights are updates after each sample. This strategy can be noisy. A middle ground is to use minibatches, where we average over enough samples to smooth out the per-sample noise, yet not so many that the advantage of averaging has diminishing returns.

To implement batch gradient descent, we backpropagate the loss gradient for each sample, calculate and accumulate @m{\frac{\partial \mathcal{L}}{\partial W}}. At the end of each batch, we average @m{\frac{\partial \mathcal{L}}{\partial W}} over the batch size and update the weights.

Alternatively, some implementations cache @m{X^{(i)}} for all samples, and perform backpropagation once over the @m{(N_x \times \text{batch_size})} @m{X^{(i)}} matrices. This requires more memory to save intermediate results and matrix by matrix operations for analytical gradient calculations.


@subsection{Visualization}

@subsubsection{The training process}

@subsubsection{The trained MLP}

@@@@@@ Add visualization to help understand the training process and what the trained MLP is doing on the input data.




##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##




@section{ConvNet}

@subsection{Architecture}

@subsubsection{Convolutional layer}

For an input with the dimension @m{N_x^{(i-1)} \times N_y^{(i-1)} \times C^{(i-1)}}, it is convolved with a kernel of size @m{K \times K \times C^{(i-1)}}. Both the input and the kernelt have the same depth, or number of channels, @m{C^{(i-1)}} The convolutional layer is defined by the following hyperparameters,
@list{
  @* @m{C}, the number of filters
  @* @m{K}, filter kernel size, where @m{K \geq 1}
  @* @m{S}, stride, displacement between kernel scans
  @* @m{P}, zero-padding around the edges of the input
}
This results in an output of dimensions @m{N_x^{(i)} \times N_y^{(i)} \times C^{(i)}}, where
@M{
  N_x^{(i)} &= (N_x^{(i-1)} - K + 2P)/S + 1 \nonumber \\
  N_y^{(i)} &= (N_y^{(i-1)} - K + 2P)/S + 1 \\
  C^{(i)} &= C \nonumber
}
Each filter has @m{K^2C^{(i-1)}+1} parameters, @m{+1} for the bias @m{b}. The number of parameters to train in this layer is @m{C^{(i)}(K^2C^{(i-1)}+1)}.

Note that the same @m{K \times K} kernel is shared across the entire input image for that filter layer. During backpropagation, every neuron in the volume will compute the gradient for its kernel weights, they are then averaged to update the single kernel.

Using similar notations as the fully-connected layer, we call the kernel the weight matrix @m{W}, the output of the convolutional layer @m{H}. For the @m{i}-th layer,

@M{
  H^{(i)}_{ij} = \sum_{p=1}^K \sum_{q=1}^K \sum_{r=1}^{C^{(i-1)}} W^{(i)}_{pqr} X^{(i-1)}_{Si+p-P, Sj+q-P, r} + b^{(i)}
}
where @m{b} is the bias, and this convolution is repeated for the @m{c = 1..C^{(i)}} kernels in this layer. The complete convolution for in this layer is then

@M{ \label{eq:conv_layer}
  H^{(i)}_{cij} = \sum_{p=1}^K \sum_{q=1}^K \sum_{r=1}^{C^{(i-1)}} W^{(i)}_{cpqr} X^{(i-1)}_{Si+p-P, Sj+q-P, r} + b^{(i)}_c
}


@b{Locally-connected layer}

Sometimes it does not make sense to use the same convolutional kernel across the entire image. When different kernel weights are used in different regions of the image, the layer is typically called a @emph{locally-connected layer}.


@b{Backpropagation}

To backpropagate the loss gradient @m{\partial \mathcal{L} / \partial H^{(i)}} to the weights @m{\partial \mathcal{L} / \partial W} and the biases @m{\partial \mathcal{L} / \partial b}, we apply the chain rule on @eqref{conv_layer},

@M{
  \frac{\partial \mathcal{L}}{\partial W^{(i)}_{cpqr}}
    &= \sum_i \sum_j \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
       \frac{\partial H^{(i)}_{cij}}{\partial W^{(i)}_{cpqr}} \nonumber \\
    &= \sum_i \sum_j \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
       X^{(i-1)}_{Si+p-P, Sj+q-P, r}
}
@M{
  \frac{\partial \mathcal{L}}{\partial b^{(i)}_c}
    = \sum_i \sum_j
      \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
      \frac{\partial H^{(i)}_{cij}}{\partial b^{(i)}_c}
    = \sum_i \sum_j \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
}
To get @m{\partial \mathcal{L} / \partial X^{(i-1)}}, we rewrite the indices of @eqref{conv_layer}
@M{
  H^{(i)}_{cij} = \sum_{l} \sum_{m} \sum_{r} W^{(i)}_{c, l+P-Si, m+P-Sj, r} X^{(i-1)}_{lmr} + b^{(i)}_c
}
then
@M{
  \frac{\partial \mathcal{L}}{\partial X^{(i-1)}_{lmr}}
    &= \sum_i \sum_j \sum_c \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
       \frac{\partial H^{(i)}_{cij}}{\partial X^{(i-1)}_{lmr}} \nonumber \\
    &= \sum_i^{N_x^{(i)}} \sum_j^{N_y^{(i)}} \sum_c^{C^{(i)}} \frac{\partial \mathcal{L}}{\partial H^{(i)}_{cij}}
       W^{(i)}_{c,l+P-Si,m+P-Sj,r}
}


@subsubsection{ReLU layer}

@M{
  f_{\text{ReLU}}(x) = \max{(x,0)}
}
@M{
  \frac{\partial f_{\text{ReLU}}}{\partial x}
    = \begin{cases}
      1, x > 0 \\
      0, x \leq 0
      \end{cases}
}


@subsubsection{Pooling layer}

The pooling layer servers two purposes:
@numlist{
  @* mitigate the network's sensitivity to minor displacements in the input picture
  @* downsamples representation
}
It operates over each activation map independently. The pooling operation can be a max or averaging operation with a kernel size @m{K}, with a stride @m{S}. The output matrix is given by

@M{
  Y_{cij} = \text{Pool} (X_{c, Si:Si+k, Sj:Sj+k})
}
where the function @m{\text{Pool}(\cdot)} can be @m{\max(\cdot)} or @m{\text{mean}(\cdot)}

The backpropagate the error gradient @m{\partial \mathcal{L}/\partial Y} to @m{\partial \mathcal{L}/\partial X},

@M{
  \frac{\partial \mathcal{L}}{\partial X_{cpq}} &=
    \sum_i \sum_j
    \frac{\partial \mathcal{L}}{\partial Y_{cij}}
    \frac{\partial Y_{cij}}{\partial X_{cpq}}
}
@M{
  \frac{\partial Y^{\text{max-pool}}_{cij}}{\partial X_{cpq}} &=
    \begin{cases}
    1, Y_{cij} = X_{cpq} \\
    0, \text{otherwise}
    \end{cases}
}
@M{
  \frac{\partial Y^{\text{average-pool}}_{cij}}{\partial X_{cpq}} &=
    \frac{X_{cpq}}{K^2}
}
where @m{c} is the channel index, @m{i = 0..N_x^{(i)}}, @m{j = 0..N_y^{(i)}}, @m{p = 0..N_x^{(i-1)}}, @m{q = 0..N_y^{(i-1)}}.


@subsubsection{Network architecture}

ConvNets are stacks of convolutional, activation, pooling, and fully-connected (FC) layers. A traditional architecture lookes like
@M{
  ( (\text{Conv} + \text{ReLU}) * N + \text{Pooling} ) * M + ( \text{FC} + \text{ReLU} ) * L + \text{Softmax}
}


@b{Relationship to fully-connected networks}

For any convolutional layer, there is an equivalent fully-connected layer that implements the same forward function. Conversely, any fully-connected layer can be cast into an equivalent convolutional format by simple introducing a kernel that is the same size as the input image. Despite the mathematical equivalence, there are advantages to using convolutional layers:

@numlist{

  @* The ability to express fully-connected layers as a convolutional layer is useful. When the input image increases in size, we can use the same ConvNet to slide across the larger images in a single forward pass, instead of running the MLP over cropped images multiple times to achieve the same result.

  @* When expressing a convolutional layer as a fully-connected layer, the FC weights matrix contains many identical blocks due to weight-sharing of the kernels that concerns only local connectivity, and the large matrix is mostly zero. Thus, the conv layer contain many fewer parameters to train.

  @* Fully-connected architecture ignores the topology of the input. ConvNet gleams features from local structures in the input 2D images, and  ensures some degree of shift, scale, and distortion invariance by its local receptive fields, shared weights, and spatial/temporal subsampling in pooling.

}

@b{Deleting pooling layers}


There are suggestions to remove pooling layers and downsample with conv layers using large strides.

@emph{Reference}
@list{
  @* J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, @link{https://arxiv.org/abs/1412.6806}{Striving for Simplicity: The All Convolutional Net}, 2015

}

@subsection{Example implementations}

@subsubsection{Rewrite MLP(32,10) for MNIST as a ConvNet}

We rewrite @link{mlp32_10}{MLP(32,10)} as a ConvNet. The resulting digit classification should perform the same. The layers in this ConvNet consist of:
@numlist{
  @* Conv: @m{K=28}, @m{S=1}, @m{C=32}, filter depth is 1, @m{28^2 \times 1 \times 32 + 32 = 25120} trainable parameters.
  @* Activation: both sigmoid and ReLU are implemented.
  @* Conv: @m{K=1}, @m{S=1}, @m{C=10}, filter depth is 32, @m{1^2 \times 32 \times 10 + 10 = 330} trainable paraemters.
  @* Softmax and cross-entropy loss.
}


@subsubsection{LeNet-5 for MNIST classification}

We implement LeNet-5 and test it on the MNIST data set. The layers in the network architecture are:
@numlist{
  @* Conv: kernel size @m{K=5}, stride @m{S=1}, depth @m{C=6}, resulting in 6 feature maps, @m{5^2 \times 6 + 6 = 156} trainable parameters.
  @* Subsampling: @m{K=2}, @m{S=2}, @m{C=6}, average-pooling, times coefficient, add bias, pass through the sigmoid function. There are @m{(1+1) \times 6 = 12} trainable parameters, for the coefficient and bias in 6 layers.
  @* Conv: @m{K=5}, @m{S=1}, @m{C=16}. The first 6 filters take from 3 contiguous subsampled feature maps, the next 6 from 4, the next 3 from 4 discontiguous subsampled feature maps, and the last filter from all 6. There are @m{5^2 \times (3 \times 6 + 4 \times 6 + 4 \times 3 + 6) + 16 = 1516} trainable parameters.
  @* Subsampling: @m{K=2}, @m{S=2}, @m{C=16}, @m{(1+1)\times 16 = 12} parameters.
  @* Conv/FC: @m{K=5}, @m{S=1}, @m{C=120}, @m{5^2 \times 16 \times 120 + 120 = 48120} trainable connections.
  @* FC: 84 nodes, @m{120 \times 84 + 84 = 10164} parameters.
  @* Output layer: Euclidean Radial Basis Function units, one for each of the 10 classes, with 84 input each:
    @M{ y_i = \sum_j (x_j - w_{ij})^2 }
}

@@@@@@ Note that since LeNet-5, the preference for the pooling layer is max-pool. Do the state of the art ConvNets use multiplication coefficient, bias, and activation functions still?


@emph{Reference}
@list{
  @* Y. LeCun, L. Bottou, Y. Bengio, P, Haffner, @link{http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf}{Gradient-Based Learning Applied to Document Recognition}, 1998
}



##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##



@section{PointNet}


@subsection{Notes to sort and add}



@subsection{Architecture}

@list{

  @* Max pooling layer: a symmetric function to aggregate information from all of the points.

  @* Local and global information combination structure

  @* Two joint alignment networks to align both input points and point features

}

@subsubsection{Symmetry function for unordered input}

There are three strategies to make a model invariant to input permutation,
@numlist{

  @* Sort the input into a @emph{canonical order}. This means to sort the input based on features that are intrinsic to the data as a preprocessing step, so that any input order will be pre-sorted according to the same criteria.

  @* Treat the unordered input as a sequence to train an RNN as the symmetrical function, augment the training data by all kinds of permutations

  @* Use a simple symmetric function to aggregate the information from each point, that is, the symmetric function produces an output vector that is invariance to the input vector's internal elements ordering.

}

The symmetric function approach is used in PointNet, as follows
@M{
  f({x_1, ..., x_N}) \approx g(h(x_1), ... h(x_N))
}
where
@M*{
  f : 2^{\mathbb{R}^N} &\mapsto \mathbb{R} \\
  h : \mathbb{R}^N &\mapsto \mathbb{R}^K \\
  g : \mathbb{R}^K \times ... \times \mathbb{R}^K &\mapsto \mathbb{R}
}
where @m{N} is the number of input points. @m{h} is approximated by a MLP. @m{g} is the symmetric function, implemented as a single variable function plus a max pooling function. The result @m{f = [f_1, ..., f_K]} after max pooling is a global feature of the input set. For classification, we can train an SVM or a MLP classifier on @m{f}. However, point segmentation requires more network structures, addressed in the sections below.


@subsubsection{Local and global information aggregation}

Before the final MLP and max pooling to form the global feature, we have the transformed per-point features. Point segmentation requires knowledge of both local and global features. Therefore, the global feature is concatenated onto the local features.


@subsubsection{Joint alignment network}

More than set order invariance, semantic labeling of a point cloud needs to be invariant under certain geometric transformations, e.g. rigid body transformations. Again, a solution is to align the input set to a canonical space before feature extraction. PointNet does it by training an affine transformation matrix @emph{T-net}. The same idea is applied to the alignment of feature space as well.


@subsection{Example implementations}

@subsubsection{MNIST as a 2D point cloud, classification, no @emph{T-net}}

Following Appendix F in the PointNet paper, we transform the MNIST image data to pixel sets by intensity thresholding for values larger than 128. Each pixel contain the @m{(x,y)} coordinates in the image. The set size is limited to 256. If there are more than 256 pixels after thresholding, randomly subsample to 256 points. If there are fewer than 256 pixels, then randomly repeat points from the set for padding.

@subsubsection{MNIST, segmentation}

For point segmentation, we will attempt to segment which pixels are the numbers and which are the background. With clean data, the segmentation is simply color based. To better test the PointNet implementation, we can add white or gray noise pixels and see how well segmentation performs.



##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##



@section{Appendix}

@subsection{Matrix derivatives}

@subsubsection{Matrix-vector multiplication}

@M{
  y_{n \times 1} = A_{n \times m}x_{m \times 1} \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
}
@M{
  \frac{\partial f}{\partial x_j}
    = \frac{\partial f}{\partial y_i} \frac{\partial y_i}{\partial x_j}
    = A_{ji} \frac{\partial f}{\partial y_i}
}
@M{
  \frac{\partial f}{\partial x} &= A^T \frac{\partial f}{\partial y} \\
  (m \times 1) &= (m \times n)(n \times 1) \nonumber
}

@subsubsection{Elementwise vector function}
@M{
  y_{n \times 1} = \omega(x_{n \times 1}) \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
}
@M{
  \frac{\partial y_i}{\partial x_j}
    = \begin{cases}
      \omega'(x_j), & i=j \\
      0, & i \neq j
      \end{cases}
    = \text{diagonal matrix}
}
@M{
  \frac{\partial f}{\partial x_j}
    = \frac{\partial y_i}{\partial x_j} \frac{\partial f}{\partial y_i}
}
@M{
  \frac{\partial f}{\partial x} &= \omega'(x) \frac{\partial f}{\partial y} \\
  (n \times 1) &= (n \times n)(n \times 1) \nonumber
}

@subsubsection{Matrix-matrix multiplication}

@M{
  Y_{m \times l} &= A_{m \times n}X_{n \times l}
}
@M{
  f(Y) : \mathbb{R}^{m \times l} \mapsto \mathbb{R}
}
Using subscript-summation nnotation to derive the gradients @m{\frac{\partial f}{\partial X}} and @m{\frac{\partial f}{\partial A}}
@M{
  \frac{\partial f}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{jr} A_{iq}
    = \frac{\partial f}{\partial Y_{ir}} A_{iq}
    = A_{iq} \frac{\partial f}{\partial Y_{ir}}
}
@M{
  \frac{\partial f}{\partial X} = A^T \frac{\partial f}{\partial Y}
}

@M{
  \frac{\partial f}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{iq} X_{rj}
    = \frac{\partial f}{\partial Y_{qj}} X_{rj}
}
@M{
  \frac{\partial f}{\partial A} = \frac{\partial f}{\partial Y} X^T
}

@subsubsection{Elementwise matrix function}
@M{
  Y_{n \times m} = \omega(X_{n \times m}) \\
  f(Y) : \mathbb{R}^{n \times m} \mapsto \mathbb{R}
}
@M{
  \frac{\partial Y_{ij}}{\partial X_{qr}}
    = \begin{cases}
      \omega'(X_{qr}), & (i = q) \& (j = r) \\
      0, & (i \neq q) \| (j \neq r)
      \end{cases}
}
@M{
  \frac{\partial f}{\partial X_{ij}} = \frac{\partial f}{\partial Y_{ij}} \omega'(X_{ij})
}

@subsection{Convolution}

We start with 1D convolution. @m{f} and @m{g} are both 1D signals. @m{f} has length @m{n}, @m{g} has length @m{m}. The convolution of the two signals is defined as
@M{
  (f*g)_i = \sum_j^m g(j) f(i-j+m/2)
}
Effectively, we are sliding the kernel @m{g} across @m{f}. At each kernel position, we multiply the overlapping values of the kernel and the image and sum the results.

Convolution is associative, e.g. if we want to convolve @m{f} with kernels @m{g} and @m{h}, then
@M{
  f*g*h = f*(g*h)
}
When the kernel sizes are much smaller than the input image size, convolving the kernels first reduces the amount of computation.



##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
##%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##
