<!DOCTYPE html>
<html>
<head>

<meta http-equiv="content-type" content="text/html;charset=UTF-8" />
<title>mousenets</title>

<script type="text/x-mathjax-config">
  contents = '';

  MathJax.Hub.Config({
    TeX: {
      Macros: {
        mymatrix: ['{\\begin{bmatrix} #1 \\end{bmatrix}}',1],
        del: ['\\partial',0],
        by: ['\\over',0]
      },
      equationNumbers: {
        autoNumber: "all"
      }
    },
  });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">

<style type="text/css">

body {
  background-color: #ffffff;
  font-family: Georgia, serif;
  width: 50em;
  margin-left: auto;
  margin-right: auto;
}

h1 {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 300%;
}

.subtitle {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 150%;
}

.author {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #808080;
  text-align: center;
  font-size: 150%;
}

h2 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -70px;
  position: relative;   /* Needed so h2 .marker position works */
}

h2 .marker {
  background-color: #6aa84f;
  position: absolute;
  top: 0.2em;
  left: -140px;
  width: 50px;
  height: 0.8em;
  border-style: none;
}

h3 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -35px;
}

h4 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
}

table.doc_table {
  border-collapse: collapse;
  padding: 0px;
  margin-left: auto;
  margin-right: auto;
}

table.doc_table td {
  border: 1px solid #c0c0c0;
  vertical-align: top;
}

table.doc_table td p {
  margin: 5px 5px;
}

table.contents_table {
  border-collapse: collapse;
  padding: 0px;
}

table.contents_table td {
  vertical-align: top;
}

table.contents_table td.col0 {
  padding-right: 10px;
  border-right: 4px solid #6aa84f;
}

td.contents_line_H2 {
  padding-left: 0px;
}

td.contents_line_H3 {
  padding-left: 30px;
}

td.contents_line_H4 {
  padding-left: 60px;
}

td.contents_line_H2.col1 {
  padding-left: 10px;
}

td.contents_line_H3.col1 {
  padding-left: 40px;
}

td.contents_line_H4.col1 {
  padding-left: 70px;
}

pre {
  color: #0000ff;
  /* background-color: #e0ffff; */
  padding: 2px;
  font-size: 110%;
}

li p {
  margin: 0px 0px;
}

tt {
  color: #0000ff;
  font-size: 110%;
}

.doc_figure {
  display: block;
  margin: auto;
}

.doc_caption {
  display: block;
  text-align: center;
  font-size: 80%;
}

</style>

</head>
<body>

<!-- Main body of document inserted here -->
<script type='text/javascript'>__the_title__='Mousenets'</script><h1 id='title'></h1>
<h1 class='author'>teenylasers</h1>
<div class='contents' id='contents'></div>
<h2 class='section_entry' id='1.'><span class='marker'></span>1. Multilayer Perceptron</h2>
<h3 class='section_entry' id='1.1.'>1.1. Notations</h3>
<h3 class='section_entry' id='1.2.'>1.2. Network structure</h3>
<p>A multilayer perceptron (MLP) is a fully connected network that sequentially consists of the input layer, the hidden layer(s), and the output layer. Each hidden or output layer is given by</p>
\[\begin{align}
  H^{(i)} &amp;= W^{(i)}X^{(i-1)} \\
  X^{(i)} &amp;= F^{(i)}(H^{(i)})
\end{align}\]
<p>for the \(i\)-th layer, where \(X^{(i-1)}\) is the output from the previous layer and the input to the current one, \(W^{(i)}\) is the weights matrix, and \(F^{(i)}\) is the activation function, typically sigmoid, ReLU, or softmax. A loss function is used to compare the MLP output \(X^(n)\) with the ground truth \(Y\) and quantify its <em>goodness</em>,</p>
\[\begin{align}
  \mathcal{L}(X^{(n)}, Y) : \mathbb{R}^K \rightarrow \mathbb{R}
\end{align}\]
<p>We use superscripts in parentheses to denote layer number, and subscripts for matrix manipulation using Einstein summation notation.</p>
<p>When we look at a layer on its own, we simplify the notation by ridding of the superscript, and rename the layer output \(Y\), </p>
\[\begin{align}
  H &amp;= WX \\
  Y &amp;= F(H)
\end{align}\]
<p>where \(Y\) is an \(N\)-dimensional vector, \(X\) is an \(N_x\)-dimensional vector, and \(W\) is an \((N \times N_x)\) matrix.</p>
<h4 class='section_entry' id='1.2.1.'>1.2.1. MLP(32,10) for MNIST</h4>
<p>For a first implementation to classify hand-written digits using the MNIST dataset, we use a 3-layer MLP. Each input image is \(28\times28\) pixels, converted to a 784-element vector for the input layer. The single hidden layer is 32-wide, using the sigmoid activation function. The output layer is 10-wide, using the softmax activation function. Therefore,</p>
\[\begin{align}
  X_0 &amp;\in \mathbb{R}^{784 \times 1} \nonumber \\
  W_1 &amp;\in \mathbb{R}^{785 \times 32} \nonumber \\
  X_1 &amp;\in \mathbb{R}^{32 \times 1} \nonumber \\
  W_2 &amp;\in \mathbb{R}^{33 \times 10} \nonumber \\
  X_2 &amp;\in \mathbb{R}^{10 \times 1}
\end{align}\]
<p>Categorical cross-entropy is used to calculate loss.</p>
<p>@@@ Add bias and its role</p>
<p>@@@ What is backpropagation results supposed to look like, how to check for correctness, how to visualize?</p>
<p>@@@ Learning rate: do you apply only at the first dLdy or at each layer in backpropagation?</p>
<h3 class='section_entry' id='1.3.'>1.3. Activation function</h3>
<h4 class='section_entry' id='1.3.1.'>1.3.1. Sigmoid function</h4>
\[\begin{align}
  \sigma(X) &amp;= \frac{1}{1+e^X} \\
  \frac{\partial\sigma}{\partial X} &amp;= \sigma(X) \left(1-\sigma(X) \right)
\end{align}\]
<h4 class='section_entry' id='1.3.2.'>1.3.2. Softmax function</h4>
\[\begin{align}
  s_j(X) &amp;= \frac{e^{X_j}}{\sum_i^{K} e^{X_i}} \\
  \frac{\partial s_j}{\partial X_k}
    &amp;= \begin{cases}
       \frac{e^{X_k}(\sum_l e^{X_l} - e^{X_k})}{(\sum_l e^{X_l})^2}, &amp; j=k \\
       \frac{-e^{X_j + X_k}}{(\sum_l e^{X_l})^2}, &amp; j \neq k
       \end{cases} \nonumber \\
    &amp;= \begin{cases}
       s(X_k)(1-s(X_k)), &amp; j=k \\
       \frac{-e^{X_j + X_k}}{(\sum_l e^{X_l})^2}, &amp; j \neq k
       \end{cases}
\end{align}\]
<p>where \(X\) is a vector of length \(K\).</p>
<h3 class='section_entry' id='1.4.'>1.4. Loss function</h3>
<h4 class='section_entry' id='1.4.1.'>1.4.1. Categorial cross-entropy loss</h4>
<p>@@@ Add interpretation of (categorical) cross-entropy loss.</p>
<p>Cross-entropy loss is </p>
\[\begin{align}
  \mathcal{L}_{CE} = -\sum_i Y_i \log(X^{(n)}_i)
\end{align}\]
<p>When \(Y\) is a one-hot encoded vector, categorial cross-entropy loss is </p>
\[\begin{align}
  \mathcal{L}_{CCE} = -\log(x_p)
\end{align}\]
\[\begin{align*}
  \frac{\partial \mathcal{L}_{CCE}}{\partial X_j}
    = \begin{cases}
      X_j, &amp; Y_j = 0 \\
      X_j - 1, &amp; Y_j = 1
      \end{cases}
\end{align*}\]
\[\begin{align}
  \frac{\partial\mathcal{L}_{CCE}}{\partial X} = X - Y
\end{align}\]
<h3 class='section_entry' id='1.5.'>1.5. Backpropagation</h3>
<p>We want to minimize the loss function \(\mathcal{L}\) using gradient descent. From the neural net's output \(X_n\) and the ground truth \(Y\), we can calculate how \(\mathcal{L}\) changes due to each component of \(X_n\), i.e. the gradient \(\partial\mathcal{L} / \partial X_n\). We apply the chain rule and work the loss gradient backwards through the net. Writing out the last two layers, we have</p>
<table class='doc_table'>
<tr><td>\[\begin{align*}H^{(n-1)} = W^{(n-1)}X^{(n-2)}\end{align*}\]
<td><td><tr><td>\[\begin{align*}X^{(n-1)} = F^{(n-1)}(H^{(n-1)}) \nonumber\end{align*}\]
<td>\[\begin{align*}\frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}\end{align*}\]
<td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial H^{(n-1)}} = \frac{\partial\mathcal{L}}{\partial X^{(n-1)}} \frac{\partial X^{(n-1)}}{\partial H^{(n-1)}}\end{align*}\]
<tr><td>\[\begin{align*}H^{(n)} = W^{(n)}X^{(n-1)}\end{align*}\]
<td>\[\begin{align*}\frac{\partial H^{(n)}}{\partial W^{(n)}}, \frac{\partial H^{(n)}}{\partial X^{(n-1)}}\end{align*}\]
<td>\[\begin{align*}
      \frac{\partial\mathcal{L}}{\partial W^{(n)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
          \frac{\partial H^{(n)}}{\partial W^{(n)}},
      \frac{\partial\mathcal{L}}{\partial X^{(n-1)}}
        = \frac{\partial\mathcal{L}}{\partial H^{(n)}}
	  \frac{\partial H^{(n)}}{\partial X^{(n-1)}}\end{align*}\]
<tr><td>\[\begin{align*}X^{(n)} = F^{(n)}(H^{(n)})\end{align*}\]
<td>\[\begin{align*}\frac{\partial X^{(n)}}{\partial H^{(n)}} = F'^{(n)}(H^{(n)})\end{align*}\]
<td>\[\begin{align*}
      \frac{\partial\mathcal{L}}{\partial H^{(n)}}
        = \frac{\partial \mathcal{L}}{\partial X^{(n)}}
	  \frac{\partial X^{(n)}}{\partial H^{(n)}}\end{align*}\]
<tr><td>\[\begin{align*}\mathcal{L}(X^{(n)}, Y)\end{align*}\]
<td><td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial X^{(n)}}\end{align*}\]

</table>
<p>Thus, for each \(i\)-th layer, backpropagation is given by </p>
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial W^{(i)}}
    &amp;= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       \left(\frac{\partial H^{(i)}}{\partial W^{(i)}}\right)^T \nonumber \\
    &amp;= \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
       (X^{(i)})^T
    \label{eq:backprop_weights} \\
  (N \times N_x) &amp;= (N \times N) (N \times 1) (1 \times N_x) \nonumber
\end{align}\]
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial X^{(i-1)}}
    &amp;= \left(\frac{\partial H^{(i)}}{\partial X^{(i-1)}}\right)^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}} \nonumber \\
    &amp;= (W^{(i)})^T
       \frac{\partial X^{(i)}}{\partial H^{(i)}}
       \frac{\partial \mathcal{L}}{\partial X^{(i)}}
    \label{eq:backprop_x} \\
  (N_x \times 1) &amp;= (N_x \times N) (N \times N) (N \times 1) \nonumber
\end{align}\]
<p>where \eqref{eq:backprop_weights} updates the weights in this layer and \eqref{eq:backprop_x} propagates the loss error to the previous layer. \eqref{eq:backprop_weights} and \eqref{eq:backprop_x} are written in the form to perform backpropagation using matrix operations. For checking matrix dimensions, \(N\) is the width, i.e. the number of nodes in this layer, it is also the width of the layer output; \(N_x\) is the width of the input to this layer.</p>
<h4 class='section_entry' id='1.5.1.'>1.5.1. Check gradient</h4>
<p>Backpropagation \eqref{eq:backprop_weights} and \eqref{eq:backprop_x} gives us analytically calculated gradient at each layer of the neural network. We can check the correctness of our implementation against numerically calculate gradients, that is, for the \(i\)-th layer, we wish to verify the loss gradient with respect to the layer input \(\frac{\partial \mathcal{L}}{\partial X^{(i-1)}}\) and the weights \(\frac{\partial \mathcal{L}}{\partial W^{(i)}}\)</p>
<p>We can perform gradient check at each layer individually, calculating \(\frac{\partial \mathcal{L}}{\partial X}\) and \(\frac{\partial \mathcal{L}}{\partial W}\) from \(\frac{\partial \mathcal{L}}{\partial Y}\), </p>
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial X_k}
    &amp;= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{\partial Y_j}{\partial X_k} \nonumber \nonumber \\
    &amp;= \sum^N_j \frac{\partial \mathcal{L}}{\partial Y_j} \frac{Y_j(X_k + \epsilon) - Y_j(X_k - \epsilon)}{2\epsilon}
\end{align}\]
<p>Alternatively, we can check gradient by tweaking the \(k\)-th element on the \(i\)-th layer in the neural net \(X^{(i)}_k\), then forward propagate its effect through the network all the way to the loss function \(\mathcal{L}\) to calculate the loss gradient, </p>
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial X^{(i)}_k}
    &amp;= \frac{\mathcal{L}(X^{(i)}_k + \epsilon) - \mathcal{L}(X^{(i)}_k - \epsilon)}{2\epsilon}
\end{align}\]
<h2 class='section_entry' id='2.'><span class='marker'></span>2. Appendix</h2>
<h3 class='section_entry' id='2.1.'>2.1. Matrix derivatives</h3>
<h4 class='section_entry' id='2.1.1.'>2.1.1. Matrix-vector multiplication</h4>
\[\begin{align}
  y_{n \times 1} = A_{n \times m}x_{m \times 1} \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial x_j}
    = \frac{\partial f}{\partial y_i} \frac{\partial y_i}{\partial x_j}
    = A_{ji} \frac{\partial f}{\partial y_i}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial x} &amp;= A^T \frac{\partial f}{\partial y} \\
  (m \times 1) &amp;= (m \times n)(n \times 1) \nonumber
\end{align}\]
<h4 class='section_entry' id='2.1.2.'>2.1.2. Elementwise vector function</h4>
\[\begin{align}
  y_{n \times 1} = \omega(x_{n \times 1}) \\
  f(y) : \mathbb{R}^{n \times 1} \mapsto \mathbb{R}
\end{align}\]
\[\begin{align}
  \frac{\partial y_i}{\partial x_j}
    = \begin{cases}
      \omega'(x_j), &amp; i=j \\
      0, &amp; i \neq j
      \end{cases}
    = \text{diagonal matrix}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial x_j}
    = \frac{\partial y_i}{\partial x_j} \frac{\partial f}{\partial y_i}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial x} &amp;= \omega'(x) \frac{\partial f}{\partial y} \\
  (n \times 1) &amp;= (n \times n)(n \times 1) \nonumber
\end{align}\]
<h4 class='section_entry' id='2.1.3.'>2.1.3. Matrix-matrix multiplication</h4>
\[\begin{align}
  Y_{m \times l} &amp;= A_{m \times n}X_{n \times l}
\end{align}\]
\[\begin{align}
  f(Y) : \mathbb{R}^{m \times l} \mapsto \mathbb{R}
\end{align}\]
<p>Using subscript-summation nnotation to derive the gradients \(\frac{\partial f}{\partial X}\) and \(\frac{\partial f}{\partial A}\) </p>
\[\begin{align}
  \frac{\partial f}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{jr} A_{iq}
    = \frac{\partial f}{\partial Y_{ir}} A_{iq}
    = A_{iq} \frac{\partial f}{\partial Y_{ir}}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial X} = A^T \frac{\partial f}{\partial Y}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{iq} X_{rj}
    = \frac{\partial f}{\partial Y_{qj}} X_{rj}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial A} = \frac{\partial f}{\partial Y} X^T
\end{align}\]
<h4 class='section_entry' id='2.1.4.'>2.1.4. Elementwise matrix function</h4>
\[\begin{align}
  y_{n \times m} = \omega(x_{n \times m}) \\
  f(y) : \mathbb{R}^{n \times m} \mapsto \mathbb{R}
\end{align}\]


<script type="text/javascript">
// Fill in contents.
var c = document.getElementById("contents");
var elements = document.getElementsByClassName('section_entry');
if (c && elements) {
  var html = '<h2>Contents</h2><table class="contents_table">\n';
  var L = elements.length + (elements.length & 1);  // Round up to even number
  for (var i = 0; i < L; i++) {
    var j = (i >> 1) + (i & 1)*(L/2);
    if ((i & 1) == 0) html += '<tr>';
    if (j < elements.length) {
      html += '<td class="col'+(i&1)+' contents_line_' + elements[j].nodeName +
              '"><a href="#' + elements[j].id + '">' + elements[j].innerHTML +
              '</a></td>';
    } else {
      html += '<td></td>';
    }
    if ((i & 1) == 1) html += '</tr>\n';
  }
  c.innerHTML = html + '</table>\n';
}

// Set title.
document.getElementById("title").innerHTML = __the_title__;
</script>

</body>
</html>
