<!DOCTYPE html>
<html>
<head>

<meta http-equiv="content-type" content="text/html;charset=UTF-8" />
<title>mousenets</title>

<script type="text/x-mathjax-config">
  contents = '';

  MathJax.Hub.Config({
    TeX: {
      Macros: {
        mymatrix: ['{\\begin{bmatrix} #1 \\end{bmatrix}}',1],
        del: ['\\partial',0],
        by: ['\\over',0]
      },
      equationNumbers: {
        autoNumber: "all"
      }
    },
  });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">

<style type="text/css">

body {
  background-color: #ffffff;
  font-family: Georgia, serif;
  width: 50em;
  margin-left: auto;
  margin-right: auto;
}

h1 {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 300%;
}

.subtitle {
  font-family: 'Roboto', sans-serif;
  color: #6aa84f;
  text-align: center;
  font-size: 150%;
}

.author {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #808080;
  text-align: center;
  font-size: 150%;
}

h2 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -70px;
  position: relative;   /* Needed so h2 .marker position works */
}

h2 .marker {
  background-color: #6aa84f;
  position: absolute;
  top: 0.2em;
  left: -140px;
  width: 50px;
  height: 0.8em;
  border-style: none;
}

h3 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
  text-indent: -35px;
}

h4 {
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  color: #6aa84f;
}

table.doc_table {
  border-collapse: collapse;
  padding: 0px;
  margin-left: auto;
  margin-right: auto;
}

table.doc_table td {
  border: 1px solid #c0c0c0;
  vertical-align: top;
}

table.doc_table td p {
  margin: 5px 5px;
}

table.contents_table {
  border-collapse: collapse;
  padding: 0px;
}

table.contents_table td {
  vertical-align: top;
}

table.contents_table td.col0 {
  padding-right: 10px;
  border-right: 4px solid #6aa84f;
}

td.contents_line_H2 {
  padding-left: 0px;
}

td.contents_line_H3 {
  padding-left: 30px;
}

td.contents_line_H4 {
  padding-left: 60px;
}

td.contents_line_H2.col1 {
  padding-left: 10px;
}

td.contents_line_H3.col1 {
  padding-left: 40px;
}

td.contents_line_H4.col1 {
  padding-left: 70px;
}

pre {
  color: #0000ff;
  /* background-color: #e0ffff; */
  padding: 2px;
  font-size: 110%;
}

li p {
  margin: 0px 0px;
}

tt {
  color: #0000ff;
  font-size: 110%;
}

.doc_figure {
  display: block;
  margin: auto;
}

.doc_caption {
  display: block;
  text-align: center;
  font-size: 80%;
}

</style>

</head>
<body>

<!-- Main body of document inserted here -->
<script type='text/javascript'>__the_title__='Mousenets'</script><h1 id='title'></h1>
<h1 class='author'>teenylasers</h1>
<div class='contents' id='contents'></div>
<h2 class='section_entry' id='1.'><span class='marker'></span>1. Multilayer Perceptron</h2>
<h3 class='section_entry' id='1.1.'>1.1. Notations</h3>
<h3 class='section_entry' id='1.2.'>1.2. Network structure</h3>
<p>A multilayer perceptron (MLP) is a fully connected network that sequentially consists of the input layer, the hidden layer(s), and the output layer. Each hidden or output layer is given by</p>
\[\begin{align}
  H_i &amp;= W_iX_{i-1} \\
  X_i &amp;= F_i(H_i)
\end{align}\]
<p>for the \(i\)-th layer, where \(X_{i-1}\) is the output from the previous layer and the input to the current one, \(W_i\) is the weights matrix, and \(F_i\) is the activation function, typically sigmoid, ReLU, or softmax. A loss function is used to compare the MLP output \(X_n\) with the ground truth \(Y\) and quantify its <em>goodness</em>,</p>
\[\begin{align}
  \mathcal{L}(X_n, Y) : \mathbb{R}^K \rightarrow \mathbb{R}
\end{align}\]
<h4 class='section_entry' id='1.2.1.'>1.2.1. MLP(32,10) for MNIST</h4>
<p>For a first implementation to classify hand-written digits using the MNIST dataset, we use a 3-layer MLP. Each input image is \(28\times28\) pixels, converted to a 784-element vector for the input layer. The single hidden layer is 32-wide, using the sigmoid activation function. The output layer is 10-wide, using the softmax activation function. Therefore,</p>
\[\begin{align}
  X_0 &amp;\in \mathbb{R}^{784 \times 1} \nonumber \\
  W_1 &amp;\in \mathbb{R}^{785 \times 32} \nonumber \\
  X_1 &amp;\in \mathbb{R}^{32 \times 1} \nonumber \\
  W_2 &amp;\in \mathbb{R}^{33 \times 10} \nonumber \\
  X_2 &amp;\in \mathbb{R}^{10 \times 1}
\end{align}\]
<p>Categorical cross-entropy is used to calculate loss.</p>
<p>@@@ Add bias and its role</p>
<p>@@@ What is backpropagation results supposed to look like, how to check for correctness, how to visualize?</p>
<p>@@@ Learning rate: do you apply only at the first dLdy or at each layer in backpropagation?</p>
<h3 class='section_entry' id='1.3.'>1.3. Activation function</h3>
<h4 class='section_entry' id='1.3.1.'>1.3.1. Sigmoid function</h4>
\[\begin{align}
  \sigma(X) &amp;= \frac{1}{1+\exp(X)} \\
  \frac{\partial\sigma}{\partial X} &amp;= \sigma(X) \left(1-\sigma(X) \right)
\end{align}\]
<h4 class='section_entry' id='1.3.2.'>1.3.2. Softmax function</h4>
\[\begin{align}
  s(X) &amp;= \frac{\exp(X)}{\sum_i^{K} \exp(X_i)} \\
  \frac{\partial s}{\partial X} &amp;= s(X) \left( 1-s(X) \right)
\end{align}\]
<p>where \(X\) is a vector of length \(K\).</p>
<h3 class='section_entry' id='1.4.'>1.4. Loss function</h3>
<h4 class='section_entry' id='1.4.1.'>1.4.1. Categorial cross-entropy loss</h4>
<h3 class='section_entry' id='1.5.'>1.5. Backpropagation</h3>
<p>We want to minimize the loss function \(\mathcal{L}\) using gradient descent. From the neural net's output \(X_n\) and the ground truth \(Y\), we can calculate how \(\mathcal{L}\) changes due to each component of \(X_n\), i.e. the gradient \(\partial\mathcal{L} / \partial X_n\). We apply the chain rule and work the loss gradient backwards through the net. Writing out the last two layers, we have</p>
<table class='doc_table'>
<tr><td>\[\begin{align*}H_{n-1} = W_{n-1}X_{n-2}\end{align*}\]
<td><td><tr><td>\[\begin{align*}X_{n-1} = F_{n-1}\left(H_{n-1}\right) \nonumber\end{align*}\]
<td>\[\begin{align*}\frac{\partial X_{n-1}}{\partial H_{n-1}}\end{align*}\]
<td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial H_{n-1}} = \frac{\partial\mathcal{L}}{\partial X_{n-1}} \frac{\partial X_{n-1}}{\partial H_{n-1}}\end{align*}\]
<tr><td>\[\begin{align*}H_n = W_nX_{n-1}\end{align*}\]
<td>\[\begin{align*}\frac{\partial H_n}{\partial W_n}, \frac{\partial H_n}{\partial X_{n-1}}\end{align*}\]
<td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial W_n} = \frac{\partial\mathcal{L}}{\partial H_n}\frac{\partial H_n}{\partial W_n}, \frac{\partial\mathcal{L}}{\partial X_{n-1}} = \frac{\partial\mathcal{L}}{\partial H_n} \frac{\partial H_n}{\partial X_{n-1}}\end{align*}\]
<tr><td>\[\begin{align*}X_n = F_n\left(H_n\right)\end{align*}\]
<td>\[\begin{align*}\frac{\partial X_n}{\partial H_n} = F_n'\left(H_n\right)\end{align*}\]
<td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial H_n} = \frac{\partial \mathcal{L}}{\partial X_n} \frac{\partial X_n}{\partial H_n}\end{align*}\]
<tr><td>\[\begin{align*}\mathcal{L}(X_n, Y)\end{align*}\]
<td><td>\[\begin{align*}\frac{\partial\mathcal{L}}{\partial X_n}\end{align*}\]

</table>
<p>Thus, for each \(i\)-th layer, backpropagation is given by </p>
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial W_i} &amp;=
    \frac{\partial X_i}{\partial H_i}
    \frac{\partial \mathcal{L}}{\partial X_i}
    \left(\frac{\partial H_i}{\partial W_i}\right)^T
    \label{eq:backprop_weights} \\
  (N \times N_x) &amp;= (N \times N) (N \times 1) (1 \times N_x) \nonumber
\end{align}\]
\[\begin{align}
  \frac{\partial \mathcal{L}}{\partial X_{i-1}} &amp;=
    \left(\frac{\partial H_i}{\partial X_{i-1}}\right)^T
    \frac{\partial X_i}{\partial H_i}
    \frac{\partial \mathcal{L}}{\partial X_i}
    \label{eq:backprop_x} \\
  (N_x \times 1) &amp;= (N_x \times N) (N \times N) (N \times 1) \nonumber
\end{align}\]
<p>where \eqref{eq:backprop_weights} updates the weights in this layer and \eqref{eq:backprop_x} propagates the loss error to the previous layer. \eqref{eq:backprop_weights} and \eqref{eq:backprop_x} are written in the form to perform backpropagation using matrix operations. For checking matrix dimensions, \(N\) is the width, i.e. the number of nodes in this layer, it is also the width of the layer output; \(N_x\) is the width of the input to this layer.</p>
<h4 class='section_entry' id='1.5.1.'>1.5.1. Check gradient</h4>
<h2 class='section_entry' id='2.'><span class='marker'></span>2. Appendix</h2>
<h3 class='section_entry' id='2.1.'>2.1. Matrix calculus</h3>
\[\begin{align}
  Y &amp;= AX \\
  (m \times l) &amp;= (m \times n)(n \times l) \nonumber
\end{align}\]
\[\begin{align}
  f(Y) : \mathbb{R}^{m \times l} \mapsto \mathbb{R}
\end{align}\]
<p>Using subscript-summation nnotation to derive the gradients \(\frac{\partial f}{\partial X}\) and \(\frac{\partial f}{\partial A}\) </p>
\[\begin{align}
  \frac{\partial f}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial X_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{jr} A_{iq}
    = \frac{\partial f}{\partial Y_{ir}} A_{iq}
    = A_{iq} \frac{\partial f}{\partial Y_{ir}}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial X} = A^T \frac{\partial f}{\partial Y}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \frac{\partial (AX)_{ij}}{\partial A_{qr}}
    = \frac{\partial f}{\partial Y_{ij}} \delta_{iq} X_{rj}
    = \frac{\partial f}{\partial Y_{qj}} X_{rj}
\end{align}\]
\[\begin{align}
  \frac{\partial f}{\partial A} = \frac{\partial f}{\partial Y} X^T
\end{align}\]


<script type="text/javascript">
// Fill in contents.
var c = document.getElementById("contents");
var elements = document.getElementsByClassName('section_entry');
if (c && elements) {
  var html = '<h2>Contents</h2><table class="contents_table">\n';
  var L = elements.length + (elements.length & 1);  // Round up to even number
  for (var i = 0; i < L; i++) {
    var j = (i >> 1) + (i & 1)*(L/2);
    if ((i & 1) == 0) html += '<tr>';
    if (j < elements.length) {
      html += '<td class="col'+(i&1)+' contents_line_' + elements[j].nodeName +
              '"><a href="#' + elements[j].id + '">' + elements[j].innerHTML +
              '</a></td>';
    } else {
      html += '<td></td>';
    }
    if ((i & 1) == 1) html += '</tr>\n';
  }
  c.innerHTML = html + '</table>\n';
}

// Set title.
document.getElementById("title").innerHTML = __the_title__;
</script>

</body>
</html>
