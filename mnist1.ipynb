{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNu0MlU9EQH2kAg/txlBAX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GN6Hq9LLPuf",
        "colab_type": "text"
      },
      "source": [
        "#MNIST Classification using Fully Connected Neural Nets\n",
        "\n",
        "This follows the same neural nets as implemented in [mnist0.ipynb](https://colab.research.google.com/drive/1gmXteU_qhI1RP6CDhEoHFcSK_MsYfqZX?authuser=1#scrollTo=Iaxyao85dvcX), but using Keras API for loading data only. The construction and training of the neural nets are custom implementations herein. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6u-1wBMW6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnoa3ENsMf_Q",
        "colab_type": "text"
      },
      "source": [
        "Load the MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvM3uVcYMadG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4144ca4b-4e91-4d05-902a-20e3beb25f30"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# x_train, x_test: uint8 arrays of grayscale image data with shapes  (num_samples, 28, 28).  \n",
        "# y_train, y_test: uint8 arrays of digit labels (integers in range 0-9) with shapes (num_samples,).  \n",
        "print('x_train (training data) dimensions: ', x_train.shape)\n",
        "print('x_test (test data) dimensions: ', x_test.shape)\n",
        "print('y_train (training labels) dimensions: ', y_train.shape)\n",
        "print('y_test (test labels) dimensions: ', y_test.shape)\n",
        "\n",
        "# Reshape 2D input images into 1D vectors.\n",
        "image_vector_length = x_train.shape[1] * x_train.shape[2]\n",
        "x_train = x_train.reshape(x_train.shape[0],image_vector_length)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_length)\n",
        "print('x_train reshaped dimensions: ', x_train.shape)\n",
        "print('x_test reshaped dimensions: ', x_test.shape)\n",
        "\n",
        "# Transform output training label into one-hot encoded vectors.\n",
        "num_classes = 10;\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train (training data) dimensions:  (60000, 28, 28)\n",
            "x_test (test data) dimensions:  (10000, 28, 28)\n",
            "y_train (training labels) dimensions:  (60000,)\n",
            "y_test (test labels) dimensions:  (10000,)\n",
            "x_train reshaped dimensions:  (60000, 784)\n",
            "x_test reshaped dimensions:  (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC-MlwqGB2in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8fe3b1cf-72bf-4158-8597-e7b72bc87c65"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# %cd drive/My\\ Drive/mousenets/mnist\n",
        "# %ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/mousenets/mnist\n",
            "__init__.py   mnist1.ipynb      mousenets.py\n",
            "mnist0.ipynb  mousenets.ipynbs  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGrH4OZ-U9Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "  \"\"\"\n",
        "  A single layer in an MLP, f(h) = f(w.dot(x))\n",
        "  \"\"\"\n",
        "  # Definitions:\n",
        "\n",
        "  # n = layer width\n",
        "  # x = input vector\n",
        "  # n_x = input vector length\n",
        "  # w = weights matrix\n",
        "  # h = intermediate variable, w.dot(x)\n",
        "  # y = output vector\n",
        "\n",
        "  # activation = activation function name\n",
        "  # f = the activation function\n",
        "  # dfdh = the derivative of the activation function\n",
        "\n",
        "  # dLdx = backpropagation from dLdy to dLdx\n",
        "  # dLdw = backpropagation from dLdy to dLdw\n",
        "\n",
        "  def __init__(self, n_x, n, activation):\n",
        "\n",
        "    assert(isinstance(n, int) and n > 0), 'n should be an integer and >0.'\n",
        "    assert(isinstance(n_x, int) and n_x > 0), 'n_x should be an integer and >0.'\n",
        "\n",
        "    self.n_x = n_x\n",
        "    self.n = n\n",
        "    self.activation = activation\n",
        "    if self.activation == 'sigmoid':\n",
        "      self.f = self._f_sigmoid\n",
        "      self.dfdh = self._dfdh_sigmoid\n",
        "    elif self.activation == \"softmax\":\n",
        "      self.f = self._f_softmax\n",
        "      self.dfdh = self._dfdh_softmax\n",
        "    else:\n",
        "      print(\"Error: activation function %s is not implemented.\",\n",
        "            self.activation)\n",
        "\n",
        "    # Initialize w as a zero matrix/vector\n",
        "    self.w = np.random.rand(self.n, self.n_x)\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    \"\"\"Saves input x as the new self.x. Calculate and return y.\"\"\"\n",
        "    assert(isinstance(x, np.ndarray)), 'x should be a numpy array.'\n",
        "    assert(len(x.shape)==1), 'x should be a vector.'\n",
        "    self.x = x\n",
        "    self.h = self.w.dot(self.x)\n",
        "    self.y = self.f(self.h)\n",
        "    return self.y\n",
        "\n",
        "  def backprop(self, dLdy):\n",
        "    \"\"\"Calculate dLdw and dLdx, update w using dLdw, return dLdx.\"\"\"\n",
        "    # dLdy = (n * 1)\n",
        "    # dydh = dfdh = (n * n) diagonal\n",
        "    # dhdx = (n * n_x) = w\n",
        "    # dhdw = (n_x * 1) = x\n",
        "    # dLdw = dydh * dLdy * dhdw.T\n",
        "    # dLdx = dhdx.T * dydh * dLdy\n",
        "    dydh = self.dfdh(self.h)\n",
        "    self.dLdw = np.outer(dydh @ dLdy, self.x.T)\n",
        "    self.dLdx = self.w.T @ dydh @ dLdy\n",
        "    self.w = self.w + self.dLdw\n",
        "    return self.dLdx\n",
        "\n",
        "  def get_layer_width(self):\n",
        "    return self.n\n",
        "\n",
        "  def get_weights(self):\n",
        "    return self.w\n",
        "\n",
        "  def _f_sigmoid(self, h):\n",
        "    \"\"\"Evaluate the sigmoid function for h, where h is a vector.\"\"\"\n",
        "    assert(len(h.shape)==1), 'Input arg h should be a vector.'\n",
        "    h = self.normalize_vector(h)\n",
        "    return 1 / (1 + np.exp(-h))\n",
        "\n",
        "  def _dfdh_sigmoid(self, h):\n",
        "    \"\"\"Evaluate the gradient of sigmoid function at h, where h is a vector.\"\"\"\n",
        "    assert(len(h.shape)==1), 'Input arg h should be a vector.'\n",
        "    f_h = self._f_sigmoid(h)\n",
        "    return np.diag((1 - f_h)*f_h)\n",
        "\n",
        "  def _f_softmax(self, h):\n",
        "    \"\"\"Evaluate the softmax function for h, where h is a vector.\"\"\"\n",
        "    assert(len(h.shape)==1), 'Input arg h should be a vector.'\n",
        "    h = self.normalize_vector(h)\n",
        "    exp_h = np.exp(h)\n",
        "    return exp_h/np.sum(exp_h)\n",
        "\n",
        "  def _dfdh_softmax(self, h):\n",
        "    \"\"\"Evaluate the gradient of softmax function at h, where h is a vector.\"\"\"\n",
        "    assert(len(h.shape)==1), 'Input arg h should be a vector.'\n",
        "    f_h = self._f_softmax(h)\n",
        "    return np.diag(f_h * (1 - f_h))\n",
        "\n",
        "  def normalize_vector(self, h):\n",
        "    \"\"\"Normalize a vector h for numerical stability\"\"\"\n",
        "    if abs(np.max(h)) != 0:\n",
        "      return h / abs(np.max(h))\n",
        "    else:\n",
        "      return h\n",
        "\n",
        "class LossFunction:\n",
        "  \"\"\"\n",
        "  Loss function, specific implementations include (categorical) cross-entropy\n",
        "  \"\"\"\n",
        "  # self.f = the loss function\n",
        "  # self.dfdx = the gradient of the loss function at x\n",
        "  def __init__(self, loss_fxn_type):\n",
        "    if loss_fxn_type == \"cce\":\n",
        "      self.f = self._f_cce\n",
        "      self.dfdx = self._dfdx_cce\n",
        "    else:\n",
        "      assert(False), 'loss function %s is not implemented.' % loss_fxn_type\n",
        "\n",
        "  def evaluate(self, x, y):\n",
        "    return self.f(x, y)\n",
        "\n",
        "  def get_gradient(self, x, y):\n",
        "    return self.dfdx(x, y)\n",
        "\n",
        "  def _f_cce(self, x, y):\n",
        "    \"\"\"Evaluate the categorical cross-entropy loss for x against the ground truth y.\"\"\"\n",
        "    # TODO: this implementation is for one-hot category only, add multi-class\n",
        "    return -1*y.dot(np.log(x))\n",
        "\n",
        "  def _dfdx_cce(self, x, y):\n",
        "    \"\"\"Evaluate the gradient of categorical cross-entropy loss at x\"\"\"\n",
        "    # TODO: this implementation is for one-hot category only, add multi-class\n",
        "    return x - y\n",
        "\n",
        "\n",
        "class MLP:\n",
        "  \"\"\"\n",
        "  A multi-layer perception\n",
        "  \"\"\"\n",
        "  # Definitions:\n",
        "  #\n",
        "  # n_x0 = input data dimension\n",
        "  # n_y = output and training data dimension\n",
        "  # x0 = MLP input, (n_x0 * 1)\n",
        "  # xn = MLP output, (n_y * 1). xn is the output of the n-th layer.\n",
        "  #      The last layer output as the same dimensions as the training data\n",
        "  # y = training data, (n_y * 1).\n",
        "  # layers = a list of Layer objects, dynamically expand using add_layer()\n",
        "  # loss = the loss function, used to evaluate the output xn\n",
        "\n",
        "  def __init__(self, input_dimension, output_dimension):\n",
        "    self.layers = []\n",
        "    self.n_x0 = input_dimension\n",
        "    self.n_y = output_dimension\n",
        "    self.x0 = [0]*self.n_x0\n",
        "    self.xn = [0]*self.n_y\n",
        "\n",
        "  def add_layer(self, n, activation):\n",
        "    \"\"\"Augment the MLP by a new layer of width n.\"\"\"\n",
        "    # Get the last layer's output dimension.\n",
        "    if len(self.layers) == 0:\n",
        "      in_dimension = self.n_x0\n",
        "    else:\n",
        "      in_dimension = self.layers[-1].get_layer_width()\n",
        "    # Automatically extend n by +1 for bias term. \n",
        "    in_dimension += 1\n",
        "    # Append the new layer.\n",
        "    self.layers.append(Layer(in_dimension, n, activation))\n",
        "\n",
        "  def define_loss_function(self, loss_fxn_type):\n",
        "    \"\"\"Define the loss function object, to evaluate the output y.\"\"\"\n",
        "    self.loss = LossFunction(loss_fxn_type)\n",
        "\n",
        "  def forward_pass(self, x0):\n",
        "    \"\"\"Perform forward pass using input x0, return output xN.\"\"\"\n",
        "    x = x0\n",
        "    for l in self.layers:\n",
        "      # Append +1 for the bias term. TODO: keep appending each time is not efficient.\n",
        "      x = np.append(x, 1)\n",
        "      # Run forward pass\n",
        "      x = l.forward_pass(x)\n",
        "    self.xn = x\n",
        "    return self.xn\n",
        "\n",
        "  def backprop(self, dLdxn):\n",
        "    \"\"\"Backpropagate loss error dLdxn to update the MLP.\"\"\"\n",
        "    dLdx = dLdxn\n",
        "    for l in reversed(self.layers):\n",
        "      dLdx = l.backprop(dLdx)\n",
        "      # The last element of dLdx is the bias term, need not be propagated to the previous layer\n",
        "      dLdx = dLdx[:-1]\n",
        "\n",
        "  def calculate_loss_gradient(self, xn, y):\n",
        "    \"\"\"Calculate dL/dxn, gradient of loss at xn from the previous data sample, using training outcome y.\"\"\"\n",
        "    return self.loss.get_gradient(xn, y)\n",
        "\n",
        "  def evaluate_loss(self, xn, y):\n",
        "    \"\"\"Evaluate the loss of a net output xn against the corresponding desired output y.\"\"\"\n",
        "    return self.loss.evaluate(xn, y)\n",
        "\n",
        "  def get_layer_width(self, i):\n",
        "    \"\"\"Get the width of the i-th layer. 0-th is the input layer, -1 for the output layer.\"\"\"\n",
        "    if i == 0:\n",
        "      return self.n_x0\n",
        "    elif i == -1:\n",
        "      return self.layers[i].get_layer_width()\n",
        "    else:\n",
        "      return self.layers[i-1].get_layer_width()\n",
        "\n",
        "  def print_weights(self, i):\n",
        "    \"\"\"Print weights for the i-th layer. 0-th is the input layer, -1 for the output layer.\"\"\"\n",
        "    if i == 0:\n",
        "      print('Weights for the input layer: N/A.\\n')\n",
        "    elif i == -1:\n",
        "      print(self.layers[i].get_weights())\n",
        "    else:\n",
        "      print(self.layers[i-1].get_weights())\n",
        "\n",
        "\n",
        "\n",
        "class NetTrainer:\n",
        "  \"\"\"\n",
        "  Train a neural net.\n",
        "  \"\"\"\n",
        "  \n",
        "  def sgd(self, nn, x_train, y_train, epochs, batch_size, eta):\n",
        "    \"\"\"Train a neural net nn using batched stochastic gradient descent.\"\"\"\n",
        "    # eta is the learning rate.\n",
        "\n",
        "    # Check input argument consistency\n",
        "    assert(isinstance(nn, MLP)), 'Input neural net nn is not an instance of MLP class.'\n",
        "\n",
        "    assert(x_train.shape[0] == y_train.shape[0]), 'x_train and y_train should have the same number of samples.'\n",
        "\n",
        "    input_width = nn.get_layer_width(0)\n",
        "    assert(x_train.shape[1] == input_width), \\\n",
        "      'x_train data has dimension %d, inconsistent with the neural net\\'s input dimension %d.' \\\n",
        "      % (x_train.shape[1], input_width)\n",
        "\n",
        "    output_width = nn.get_layer_width(-1)\n",
        "    assert(y_train.shape[1] == output_width), \\\n",
        "      'y_train data has dimension %d, inconsistent with the neural net\\'s output dimension %d.' \\\n",
        "      % (y_train.shape[1], output_width)\n",
        "\n",
        "    num_samples = x_train.shape[0]\n",
        "    assert(batch_size <= num_samples), 'batch_size [%d] > number of samples in x/y_train [%d].' \\\n",
        "      % (batch_size, num_samples)\n",
        "\n",
        "    # Do batched sgd\n",
        "    for i in range(epochs):\n",
        "      cumulative_loss = 0\n",
        "      cumulative_loss_gradient = [0]*output_width\n",
        "\n",
        "      # Evaluate loss and loss gradient for a batch\n",
        "      for j in range(batch_size):\n",
        "        s = self._select_sample(j, num_samples)\n",
        "        res = nn.forward_pass(x_train[s])\n",
        "        cumulative_loss += nn.evaluate_loss(res, y_train[s])\n",
        "        cumulative_loss_gradient += nn.calculate_loss_gradient(res, y_train[s])\n",
        "\n",
        "      # Train for this epoch\n",
        "      cumulative_loss = cumulative_loss / batch_size\n",
        "      cumulative_loss_gradient = cumulative_loss_gradient / batch_size\n",
        "      nn.backprop(cumulative_loss_gradient * eta)\n",
        "      #nn.print_weights(1)\n",
        "      #nn.print_weights(2)\n",
        "      print('Epoch #%d: loss = %f\\n' % (i, cumulative_loss))\n",
        "\n",
        "  def _select_sample(self, count, num_samples):\n",
        "    \"\"\"Helper function to select sample from num_samples.\"\"\"\n",
        "    # Currently round-robin across all num_samples. Can also select at random.\n",
        "    return count % num_samples\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljadyRmgkKtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d743ff0d-abf1-468b-bbd1-99f64e309e5c"
      },
      "source": [
        "# Construct the MLP\n",
        "l1_width = 32 # the first layer has 32 nodes\n",
        "l2_width = 10 # the second and output layer has 10 nodes\n",
        "mlp = MLP(input_dimension = image_vector_length, output_dimension = num_classes)\n",
        "mlp.add_layer(l1_width, 'sigmoid')\n",
        "mlp.add_layer(l2_width, 'softmax')\n",
        "mlp.define_loss_function('cce')\n",
        "\n",
        "# Train the MLP using sgd\n",
        "trainer = NetTrainer()\n",
        "trainer.sgd(mlp, x_train, y_train, epochs=5, batch_size=128, eta=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0: loss = 2.303992\n",
            "\n",
            "Epoch #1: loss = 2.305385\n",
            "\n",
            "Epoch #2: loss = 2.306727\n",
            "\n",
            "Epoch #3: loss = 2.308015\n",
            "\n",
            "Epoch #4: loss = 2.309258\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7zvsQV7v9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}